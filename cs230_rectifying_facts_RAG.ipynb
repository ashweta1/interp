{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM8+W9gCFJ8EKzC91+a0zgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashweta1/interp/blob/main/cs230_rectifying_facts_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rectifying Factual knowledge through RAG (Retrieval Augmented Generation)\n",
        "\n",
        "This colab uses RAG on wikipedia knowledge dataset, to prepend context to prompts.\n",
        "\n",
        "RAG is used from the library I wrote: git+https://github.com/ashweta1/rag_wiki.git\n",
        "\n",
        "Dataset used for evaluation:  https://github.com/kmeng01/rome/tree/main/dsets\n"
      ],
      "metadata": {
        "id": "PM-7oKYqN21I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare environment"
      ],
      "metadata": {
        "id": "GxrQjNdXRQHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# check that colab exists\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "\n",
        "# recreate the local home for this colab run\n",
        "cd /content && rm -rf /content/home && mkdir home && cd home\n",
        "\n",
        "# install the known facts dataset.\n",
        "pip install git+https://github.com/kmeng01/rome.git/tree/main/dsets >> install.log 2>&1\n",
        "\n",
        "# install hugging face datasets library\n",
        "pip install datasets >> install.log 2>&1\n",
        "\n",
        "pip install git+https://github.com/ashweta1/rag_wiki.git >> install.log 2>&1\n",
        "pip list | grep rag_wiki\n",
        "\n",
        "# install latest torch and faiss-gpu\n",
        "pip uninstall -y torch faiss-cpu faiss-gpu >> install.log 2>&1\n",
        "pip install torch faiss-gpu >> install.log 2>&1\n",
        "\n",
        "# pip uninstall -y torch torchaudio torchvision torchtext torchdata faiss-gpu >> install.log 2>&1\n",
        "# pip install torch torchaudio torchvision torchtext torchdata faiss-gpu >> install.log 2>&1"
      ],
      "metadata": {
        "id": "hRdxVUS8PcXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "      device = torch.device(\"mps\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    print(\"Device = \", device)\n",
        "        # raise Exception(\"Change runtime type to include a GPU.\")\n",
        "\n",
        "    os.chdir(\"/content/home\")\n",
        "    torch.set_grad_enabled(False)  # no model parameter updates\n",
        "\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ],
      "metadata": {
        "id": "7IKG7TkLRT5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# # Install my own rag_wiki library to use RAG from wikipeda\n",
        "# cd /content/home && rm -fr /content/home/rag_wiki\n",
        "\n",
        "# git clone https://github.com/ashweta1/rag_wiki.git && cd /content/home/rag_wiki && pip install -e . && cd /content/home\n",
        "# ls -R /content/home/rag_wiki/\n"
      ],
      "metadata": {
        "id": "5AXMWoMevmqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IPYTHON magic to automatically reload imported module if they change\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "metadata": {
        "id": "Npy1iKe0R-id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "FdO0Ztqp8Ya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get wikipedia embeddings loaded\n",
        "import torch\n",
        "from rag_wiki import rag\n",
        "\n",
        "print(\"torch.cuda.is_available()\", torch.cuda.is_available())\n",
        "print(torch.__version__)\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = rag.load_wiki_dataset(num_examples=100, debug=True)\n",
        "print(\"Loading dataset...done\")\n",
        "print(\"\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "print(\"Preprocessing dataset...\")\n",
        "index, texts = rag.preprocess(dataset, batch_size=200, debug=True)\n",
        "print(\"Preprocessing dataset...done\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "d6BGvIuzwVI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yIfTxzTdSlqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the index and retrieve relevant texts\n",
        "TOP_K_TEXTS = 1\n",
        "prompts = [\"What is the capital of India?\",\n",
        "           \"Who is the president of the United States?\",\n",
        "           \"What is the population of China?\",\n",
        "           \"The captial of France is \",\n",
        "           \"Where is the Eiffel Tower located?\"]\n",
        "\n",
        "print(\"Retrieving relevant texts...\")\n",
        "print(\"Index: \", index)\n",
        "print(\"Length of texts = \", len(texts))\n",
        "retrieved_texts = rag.retrieve(prompts, index, texts, top_k=TOP_K_TEXTS, debug=False)\n",
        "print(\"Retrieving relevant texts...done\")\n",
        "print(\"\")\n",
        "\n",
        "for p, ts in zip(prompts, retrieved_texts):\n",
        "    print(f\"Prompt: {p}\")\n",
        "    print(f\"Retrieved texts: {ts}\")\n",
        "    print(\"\")\n",
        "\n",
        "contexts = [f\"{' '.join(ts)} {p}\" for p, ts in zip(prompts, retrieved_texts)]\n",
        "print(\"Prompts with contexts: \", contexts)"
      ],
      "metadata": {
        "id": "UJRQO7v0107e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "def get_gpt2_model():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id).to(device)\n",
        "    return model, tokenizer\n",
        "model, tokenizer = get_gpt2_model()"
      ],
      "metadata": {
        "id": "q6bOKGbsGMJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=50, device=device):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    outputs = model.generate(input_ids.to(device),\n",
        "                             max_length=50,\n",
        "                             do_sample=True,\n",
        "                             num_beams=2,\n",
        "                             temperature=0.001,\n",
        "                             no_repeat_ngram_size=2,\n",
        "                             early_stopping=True,\n",
        "                             eos_token_id=tokenizer.encode(\".\")[0])\n",
        "\n",
        "    # Decode the generated sequence back to text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "shjK3eEiGMlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, \"The capital of France is\")"
      ],
      "metadata": {
        "id": "c_CNr4CWGQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, \"The capital of India is\")"
      ],
      "metadata": {
        "id": "h6JbkuXOGS0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in prompts:\n",
        "    print(p)\n",
        "    generate_text(model, tokenizer, p)"
      ],
      "metadata": {
        "id": "yVgEXTtgGdIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in contexts:\n",
        "    print(generate_text(model, tokenizer, p))"
      ],
      "metadata": {
        "id": "Vt6xKAF33PJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}